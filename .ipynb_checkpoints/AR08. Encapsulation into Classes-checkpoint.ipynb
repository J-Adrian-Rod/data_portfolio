{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encapsulation into Classes\n",
    "\n",
    "\n",
    "## The `CasesModel` class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "plt.style.use('dashboard.mplstyle')\n",
    "\n",
    "GROUPS = 'world', 'usa'\n",
    "KINDS = 'cases', 'deaths'\n",
    "MIN_OBS = 15  # Minimum observations needed to make prediction\n",
    "\n",
    "def general_logistic_shift(x, L, x0, k, v, s):\n",
    "    return (L - s) / ((1 + np.exp(-k * (x - x0))) ** (1 / v)) + s\n",
    "\n",
    "def optimize_func(params, x, y, model):\n",
    "    y_pred = model(x, *params)\n",
    "    error = y - y_pred\n",
    "    return error\n",
    "\n",
    "class CasesModel:\n",
    "    def __init__(self, model, data, last_date, n_train, n_smooth, \n",
    "                 n_pred, L_n_min, L_n_max, **kwargs):\n",
    "        \"\"\"\n",
    "        Smooths, trains, and predicts cases for all areas\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : function such as general_logistic_shift\n",
    "        \n",
    "        data : dictionary of data from all areas - result of PrepareData().run()\n",
    "        \n",
    "        last_date : str, last date to be used for training\n",
    "        \n",
    "        n_train : int, number of preceding days to use for training\n",
    "        \n",
    "        n_smooth : integer, number of points used in LOWESS\n",
    "        \n",
    "        n_pred : int, days of predictions to make\n",
    "        \n",
    "        L_n_min, L_n_max : int, min/max number of days used to estimate L_min/L_max\n",
    "        \n",
    "        **kwargs : extra keyword arguments passed to scipy's least_squares function\n",
    "        \"\"\"\n",
    "        # Set basic attributes\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.last_date = self.get_last_date(last_date)\n",
    "        self.n_train = n_train\n",
    "        self.n_smooth = n_smooth\n",
    "        self.n_pred = n_pred\n",
    "        self.L_n_min = L_n_min\n",
    "        self.L_n_max = L_n_max\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        # Set attributes for prediction\n",
    "        self.first_pred_date = pd.Timestamp(self.last_date) + pd.Timedelta(\"1D\")\n",
    "        self.pred_index = pd.date_range(start=self.first_pred_date, periods=n_pred)\n",
    "        \n",
    "    def get_last_date(self, last_date):\n",
    "        # Use the most current date as the last actual date if not provided\n",
    "        if last_date is None:\n",
    "            return self.data['world_cases'].index[-1]\n",
    "        else:\n",
    "            return pd.Timestamp(last_date)\n",
    "        \n",
    "    def init_dictionaries(self):\n",
    "        # Create dictionaries to store results for each area\n",
    "        # Executed first in `run` method\n",
    "        self.smoothed = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.bounds = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.p0 = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.params = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.pred_daily = {'world_cases': {}, 'usa_cases': {}}\n",
    "        self.pred_cumulative = {'world_cases': {}, 'usa_cases': {}}\n",
    "        \n",
    "        # Dictionary to hold DataFrame of actual and predicted values\n",
    "        self.combined_daily = {}\n",
    "        self.combined_cumulative = {}\n",
    "        \n",
    "        # Same as above, but stores smoothed and predicted values\n",
    "        self.combined_daily_s = {}\n",
    "        self.combined_cumulative_s = {}\n",
    "        \n",
    "    def smooth(self, s):\n",
    "        s = s[:self.last_date]\n",
    "        if s.values[0] == 0:\n",
    "            # Filter the data if the first value is 0\n",
    "            last_zero_date = s[s == 0].index[-1]\n",
    "            s = s.loc[last_zero_date:]\n",
    "            s_daily = s.diff().dropna()\n",
    "        else:\n",
    "            # If first value not 0, use it to fill in the \n",
    "            # first missing value\n",
    "            s_daily = s.diff().fillna(s.iloc[0])\n",
    "\n",
    "        # Don't smooth data with less than MIN_OBS values\n",
    "        if len(s_daily) < MIN_OBS:\n",
    "            return s_daily.cumsum()\n",
    "\n",
    "        y = s_daily.values\n",
    "        frac = self.n_smooth / len(y)\n",
    "        x = np.arange(len(y))\n",
    "        y_pred = lowess(y, x, frac=frac, is_sorted=True, return_sorted=False)\n",
    "        s_pred = pd.Series(y_pred, index=s_daily.index).clip(0)\n",
    "        s_pred_cumulative = s_pred.cumsum()\n",
    "        \n",
    "        if s_pred_cumulative[-1]  == 0:\n",
    "            # Don't use smoothed values if they are all 0\n",
    "            return s_daily.cumsum()\n",
    "        \n",
    "        last_actual = s.values[-1]\n",
    "        last_smoothed = s_pred_cumulative.values[-1]\n",
    "        s_pred_cumulative *= last_actual / last_smoothed\n",
    "        return s_pred_cumulative\n",
    "    \n",
    "    def get_train(self, smoothed):\n",
    "        # Filter the data for the most recent to capture new waves\n",
    "        return smoothed.iloc[-self.n_train:]\n",
    "    \n",
    "    def get_L_limits(self, s):\n",
    "        last_val = s[-1]\n",
    "        last_pct = s.pct_change()[-1] + 1\n",
    "        L_min = last_val * last_pct ** self.L_n_min\n",
    "        L_max = last_val * last_pct ** self.L_n_max + 1\n",
    "        L0 = (L_max - L_min) / 2 + L_min\n",
    "        return L_min, L_max, L0\n",
    "    \n",
    "    def get_bounds_p0(self, s):\n",
    "        L_min, L_max, L0 = self.get_L_limits(s)\n",
    "        x0_min, x0_max = -50, 50\n",
    "        k_min, k_max = 0.01, 0.5\n",
    "        v_min, v_max = 0.01, 2\n",
    "        s_min, s_max = 0, s[-1] + 0.01\n",
    "        s0 = s_max / 2\n",
    "        lower = L_min, x0_min, k_min, v_min, s_min\n",
    "        upper = L_max, x0_max, k_max, v_max, s_max\n",
    "        bounds = lower, upper\n",
    "        p0 = L0, 0, 0.1, 0.1, s0\n",
    "        return bounds, p0\n",
    "    \n",
    "    def train_model(self, s, bounds, p0):\n",
    "        y = s.values\n",
    "        n_train = len(y)\n",
    "        x = np.arange(n_train)\n",
    "        res = least_squares(optimize_func, p0, args=(x, y, self.model), bounds=bounds, **self.kwargs)\n",
    "        return res.x\n",
    "    \n",
    "    def get_pred_daily(self, n_train, params):\n",
    "        x_pred = np.arange(n_train - 1, n_train + self.n_pred)\n",
    "        y_pred = self.model(x_pred, *params)\n",
    "        y_pred_daily = np.diff(y_pred)\n",
    "        return pd.Series(y_pred_daily, index=self.pred_index)\n",
    "    \n",
    "    def get_pred_cumulative(self, s, pred_daily):\n",
    "        last_actual_value = s.loc[self.last_date]\n",
    "        return pred_daily.cumsum() + last_actual_value\n",
    "    \n",
    "    def convert_to_df(self, gk):\n",
    "        # convert dictionary of areas mapped to Series to DataFrames\n",
    "        self.smoothed[gk] = pd.DataFrame(self.smoothed[gk]).fillna(0).astype('int')\n",
    "        self.bounds[gk] = pd.concat(self.bounds[gk].values(), \n",
    "                                    keys=self.bounds[gk].keys()).T\n",
    "        self.bounds[gk].loc['L'] = self.bounds[gk].loc['L'].round()\n",
    "        self.p0[gk] = pd.DataFrame(self.p0[gk], index=['L', 'x0', 'k', 'v', 's'])\n",
    "        self.p0[gk].loc['L'] = self.p0[gk].loc['L'].round()\n",
    "        self.params[gk] = pd.DataFrame(self.params[gk], index=['L', 'x0', 'k', 'v', 's'])\n",
    "        self.pred_daily[gk] = pd.DataFrame(self.pred_daily[gk])\n",
    "        self.pred_cumulative[gk] = pd.DataFrame(self.pred_cumulative[gk])\n",
    "        \n",
    "    def combine_actual_with_pred(self):\n",
    "        for gk, df_pred in self.pred_cumulative.items():\n",
    "            df_actual = self.data[gk][:self.last_date]\n",
    "            df_comb = pd.concat((df_actual, df_pred))\n",
    "            self.combined_cumulative[gk] = df_comb\n",
    "            self.combined_daily[gk] = df_comb.diff().fillna(df_comb.iloc[0]).astype('int')\n",
    "            \n",
    "            df_comb_smooth = pd.concat((self.smoothed[gk], df_pred))\n",
    "            self.combined_cumulative_s[gk] = df_comb_smooth\n",
    "            self.combined_daily_s[gk] = df_comb_smooth.diff().fillna(df_comb.iloc[0]).astype('int')\n",
    "\n",
    "    def run(self):\n",
    "        self.init_dictionaries()\n",
    "        for group in GROUPS:\n",
    "            gk = f'{group}_cases'\n",
    "            df_cases = self.data[gk]\n",
    "            for area, s in df_cases.items():\n",
    "                smoothed = self.smooth(s)\n",
    "                train = self.get_train(smoothed)\n",
    "                n_train = len(train)\n",
    "                if n_train < MIN_OBS:\n",
    "                    bounds = np.full((2, 5), np.nan)\n",
    "                    p0 = np.full(5, np.nan)\n",
    "                    params = np.full(5, np.nan)\n",
    "                    pred_daily = pd.Series(np.zeros(self.n_pred), index=self.pred_index)\n",
    "                else:\n",
    "                    bounds, p0 = self.get_bounds_p0(train)\n",
    "                    params = self.train_model(train, bounds=bounds,  p0=p0)\n",
    "                    pred_daily = self.get_pred_daily(n_train, params).round(0)\n",
    "                pred_cumulative = self.get_pred_cumulative(s, pred_daily)\n",
    "                \n",
    "                # save results to dictionaries mapping each area to its result\n",
    "                self.smoothed[gk][area] = smoothed\n",
    "                self.bounds[gk][area] = pd.DataFrame(bounds, index=['lower', 'upper'], \n",
    "                                                     columns=['L', 'x0', 'k', 'v', 's'])\n",
    "                self.p0[gk][area] = p0\n",
    "                self.params[gk][area] = params\n",
    "                self.pred_daily[gk][area] = pred_daily.astype('int')\n",
    "                self.pred_cumulative[gk][area] = pred_cumulative.astype('int')\n",
    "            self.convert_to_df(gk)\n",
    "        self.combine_actual_with_pred()\n",
    "        \n",
    "    def plot_prediction(self, group, area, **kwargs):\n",
    "        group_kind = f'{group}_cases'\n",
    "        actual = self.data[group_kind][area]\n",
    "        pred = self.pred_cumulative[group_kind][area]\n",
    "        first_date = self.last_date - pd.Timedelta(self.n_train, 'D')\n",
    "        last_pred_date = self.last_date + pd.Timedelta(self.n_pred, 'D')\n",
    "        actual.loc[first_date:last_pred_date].plot(label='Actual', **kwargs)\n",
    "        pred.plot(label='Predicted').legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare import PrepareData\n",
    "data = PrepareData(download_new=False).run()\n",
    "cm = CasesModel(model=general_logistic_shift, data=data, last_date='2020-11-05', \n",
    "                n_train=60, n_smooth=15, n_pred=30, L_n_min=5, L_n_max=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/7dv9f0r12dgdq2jhl_yjy8g80000gn/T/ipykernel_63729/4160007637.py:13: RuntimeWarning: overflow encountered in power\n",
      "  return (L - s) / ((1 + np.exp(-k * (x - x0))) ** (1 / v)) + s\n",
      "/var/folders/1x/7dv9f0r12dgdq2jhl_yjy8g80000gn/T/ipykernel_63729/4160007637.py:13: RuntimeWarning: overflow encountered in power\n",
      "  return (L - s) / ((1 + np.exp(-k * (x - x0))) ** (1 / v)) + s\n",
      "/var/folders/1x/7dv9f0r12dgdq2jhl_yjy8g80000gn/T/ipykernel_63729/4160007637.py:13: RuntimeWarning: overflow encountered in power\n",
      "  return (L - s) / ((1 + np.exp(-k * (x - x0))) ** (1 / v)) + s\n",
      "/var/folders/1x/7dv9f0r12dgdq2jhl_yjy8g80000gn/T/ipykernel_63729/4160007637.py:13: RuntimeWarning: overflow encountered in power\n",
      "  return (L - s) / ((1 + np.exp(-k * (x - x0))) ** (1 / v)) + s\n"
     ]
    }
   ],
   "source": [
    "cm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class to model deaths\n",
    "\n",
    "We create another class, `DeathsModel`, to model the deaths of each area. It allows the user to set the `lag`, number of days between cases and deaths, and the `period`, number of days to tabulate the total cases/deaths for the CFR calculation. The `predict` method multiplies the CFR by the number of cases that happened `lag` days ago. For example, if we want to predict the number of deaths on November 6, we look back at the number of cases on October 22 (assuming the lag is 15) and multiply this number by the CFR of that area. To help get smoother results, we use a 7-day rolling average instead of the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeathsModel:\n",
    "    def __init__(self, data, last_date, cm, lag, period):\n",
    "        \"\"\"\n",
    "        Build simple model based on CFR to predict deaths for all areas\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dictionary of data from all areas - result of PrepareData().run()\n",
    "\n",
    "        last_date : str, last date to be used for training\n",
    "\n",
    "        cm : CasesModel instance after calling `run` method\n",
    "        \n",
    "        lag : int, number of days between cases and deaths, used to calculate CFR\n",
    "        \n",
    "        period : int, window size of number of days to calculate CFR\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.last_date = self.get_last_date(last_date)\n",
    "        self.cm = cm\n",
    "        self.lag = lag\n",
    "        self.period = period\n",
    "        self.pred_daily = {}\n",
    "        self.pred_cumulative = {}\n",
    "        \n",
    "        # Dictionary to hold DataFrame of actual and predicted values\n",
    "        self.combined_daily = {}\n",
    "        self.combined_cumulative = {}\n",
    "        \n",
    "    def get_last_date(self, last_date):\n",
    "        if last_date is None:\n",
    "            return self.data['world_cases'].index[-1]\n",
    "        else:\n",
    "            return pd.Timestamp(last_date)\n",
    "        \n",
    "    def calculate_cfr(self):\n",
    "        first_day_deaths = self.last_date - pd.Timedelta(f'{self.period}D')\n",
    "        last_day_cases = self.last_date - pd.Timedelta(f'{self.lag}D')\n",
    "        first_day_cases = last_day_cases - pd.Timedelta(f'{self.period}D')\n",
    "\n",
    "        cfr = {}\n",
    "        for group in GROUPS:\n",
    "            deaths = self.data[f'{group}_deaths']\n",
    "            cases = self.data[f'{group}_cases']\n",
    "            deaths_total = deaths.loc[self.last_date] - deaths.loc[first_day_deaths]\n",
    "            cases_total = cases.loc[last_day_cases] - cases.loc[first_day_cases]\n",
    "            cfr[group] = (deaths_total / cases_total).fillna(0.01)\n",
    "        return cfr\n",
    "    \n",
    "    def run(self):\n",
    "        self.cfr = self.calculate_cfr()\n",
    "        for group in GROUPS:\n",
    "            group_cases = f'{group}_cases'\n",
    "            group_deaths = f'{group}_deaths'\n",
    "            cfr_start_date = self.last_date - pd.Timedelta(f'{self.lag}D')\n",
    "            \n",
    "            daily_cases_smoothed = self.cm.combined_daily_s[group_cases]\n",
    "            pred_daily = daily_cases_smoothed[cfr_start_date:] * self.cfr[group]\n",
    "            pred_daily = pred_daily.iloc[:self.cm.n_pred]\n",
    "            pred_daily.index = self.cm.pred_daily[group_cases].index\n",
    "            \n",
    "            # Use repeated rolling average to smooth out the predicted deaths\n",
    "            for i in range(5):\n",
    "                pred_daily = pred_daily.rolling(14, min_periods=1, center=True).mean()\n",
    "            \n",
    "            pred_daily = pred_daily.round(0).astype(\"int\")\n",
    "            self.pred_daily[group_deaths] = pred_daily\n",
    "            last_deaths = self.data[group_deaths].loc[self.last_date]\n",
    "            self.pred_cumulative[group_deaths] = pred_daily.cumsum() + last_deaths\n",
    "        self.combine_actual_with_pred()\n",
    "            \n",
    "    def combine_actual_with_pred(self):\n",
    "        for gk, df_pred in self.pred_cumulative.items():\n",
    "            df_actual = self.data[gk][:self.last_date]\n",
    "            df_comb = pd.concat((df_actual, df_pred))\n",
    "            self.combined_cumulative[gk] = df_comb\n",
    "            self.combined_daily[gk] = df_comb.diff().fillna(df_comb.iloc[0]).astype('int')\n",
    "            \n",
    "    def plot_prediction(self, group, area, **kwargs):\n",
    "        group_kind = f'{group}_deaths'\n",
    "        actual = self.data[group_kind][area]\n",
    "        pred = self.pred_cumulative[group_kind][area]\n",
    "        first_date = self.last_date - pd.Timedelta(60, 'D')\n",
    "        last_pred_date = self.last_date + pd.Timedelta(30, 'D')\n",
    "        actual.loc[first_date:last_pred_date].plot(label='Actual', **kwargs)\n",
    "        pred.plot(label='Predicted').legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above code is placed in a function that accepts instances of the `CasesModel` and `DeathsModel` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_data(cm, dm):\n",
    "    # Get Daily Cases and Deaths\n",
    "    world_cases_d = cm.combined_daily['world_cases']\n",
    "    usa_cases_d = cm.combined_daily['usa_cases']\n",
    "    world_deaths_d = dm.combined_daily['world_deaths']\n",
    "    usa_deaths_d = dm.combined_daily['usa_deaths']\n",
    "\n",
    "    # Add USA to world \n",
    "    world_cases_d = world_cases_d.assign(USA=usa_cases_d.sum(axis=1))\n",
    "    world_deaths_d = world_deaths_d.assign(USA=usa_deaths_d.sum(axis=1))\n",
    "\n",
    "    # Get Cumulative Cases and Deaths\n",
    "    world_cases_c = cm.combined_cumulative['world_cases']\n",
    "    usa_cases_c = cm.combined_cumulative['usa_cases']\n",
    "    world_deaths_c = dm.combined_cumulative['world_deaths']\n",
    "    usa_deaths_c = dm.combined_cumulative['usa_deaths']\n",
    "\n",
    "    # Add USA to world \n",
    "    world_cases_c = world_cases_c.assign(USA=usa_cases_c.sum(axis=1))\n",
    "    world_deaths_c = world_deaths_c.assign(USA=usa_deaths_c.sum(axis=1))\n",
    "    \n",
    "    df_world = pd.concat((world_deaths_d.stack(), world_cases_d.stack(), \n",
    "                          world_deaths_c.stack(), world_cases_c.stack()), axis=1, \n",
    "                         keys=['Daily Deaths', 'Daily Cases', 'Deaths', 'Cases'])\n",
    "    \n",
    "    df_usa = pd.concat((usa_deaths_d.stack(), usa_cases_d.stack(), \n",
    "                        usa_deaths_c.stack(), usa_cases_c.stack()), axis=1, \n",
    "                       keys=['Daily Deaths', 'Daily Cases', 'Deaths', 'Cases'])\n",
    "    df_all = pd.concat((df_world, df_usa), keys=['world', 'usa'], \n",
    "                       names=['group', 'date', 'area'])\n",
    "    df_all.to_csv('data/all_data.csv')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in a file called population.csv that has the population and code (used in the map) of each area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>area</th>\n",
       "      <th>code</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>world</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38.928341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>2.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>43.851043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>world</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AND</td>\n",
       "      <td>0.077265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world</td>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>32.866268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group         area code  population\n",
       "0  world  Afghanistan  AFG   38.928341\n",
       "1  world      Albania  ALB    2.877800\n",
       "2  world      Algeria  DZA   43.851043\n",
       "3  world      Andorra  AND    0.077265\n",
       "4  world       Angola  AGO   32.866268"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop = pd.read_csv(\"data/population.csv\")\n",
    "pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge these two tables together and add columns for deaths and cases per million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>area</th>\n",
       "      <th>Daily Deaths</th>\n",
       "      <th>Daily Cases</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Cases</th>\n",
       "      <th>code</th>\n",
       "      <th>population</th>\n",
       "      <th>Deaths per Million</th>\n",
       "      <th>Cases per Million</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>world</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>1548</td>\n",
       "      <td>41814</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38.928341</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world</td>\n",
       "      <td>Albania</td>\n",
       "      <td>7</td>\n",
       "      <td>421</td>\n",
       "      <td>543</td>\n",
       "      <td>22721</td>\n",
       "      <td>ALB</td>\n",
       "      <td>2.877800</td>\n",
       "      <td>189.0</td>\n",
       "      <td>7900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>12</td>\n",
       "      <td>642</td>\n",
       "      <td>2011</td>\n",
       "      <td>60169</td>\n",
       "      <td>DZA</td>\n",
       "      <td>43.851043</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>world</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>5135</td>\n",
       "      <td>AND</td>\n",
       "      <td>0.077265</td>\n",
       "      <td>971.0</td>\n",
       "      <td>66460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world</td>\n",
       "      <td>Angola</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>299</td>\n",
       "      <td>12102</td>\n",
       "      <td>AGO</td>\n",
       "      <td>32.866268</td>\n",
       "      <td>9.0</td>\n",
       "      <td>370.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group         area  Daily Deaths  Daily Cases  Deaths  Cases code  \\\n",
       "0  world  Afghanistan             4           86    1548  41814  AFG   \n",
       "1  world      Albania             7          421     543  22721  ALB   \n",
       "2  world      Algeria            12          642    2011  60169  DZA   \n",
       "3  world      Andorra             0           90      75   5135  AND   \n",
       "4  world       Angola             3          289     299  12102  AGO   \n",
       "\n",
       "   population  Deaths per Million  Cases per Million  \n",
       "0   38.928341                40.0             1070.0  \n",
       "1    2.877800               189.0             7900.0  \n",
       "2   43.851043                46.0             1370.0  \n",
       "3    0.077265               971.0            66460.0  \n",
       "4   32.866268                 9.0              370.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = df_summary.merge(pop, how='left', on=['group','area'])\n",
    "df_summary[\"Deaths per Million\"] = (df_summary[\"Deaths\"] / df_summary[\"population\"]).round(0)\n",
    "df_summary[\"Cases per Million\"] = (df_summary[\"Cases\"] / df_summary[\"population\"]).round(-1)\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's place all of this code within its own function which also writes the data to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_table(df_all, last_date):\n",
    "    df = df_all.query('date == @last_date')\n",
    "    pop = pd.read_csv(\"data/population.csv\")\n",
    "    df = df.merge(pop, how='left', on=['group','area'])\n",
    "    df[\"Deaths per Million\"] = (df[\"Deaths\"] / df[\"population\"]).round(0)\n",
    "    df[\"Cases per Million\"] = (df[\"Cases\"] / df[\"population\"]).round(-1)\n",
    "    df['date'] = last_date\n",
    "    df.to_csv('data/summary.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>area</th>\n",
       "      <th>Daily Deaths</th>\n",
       "      <th>Daily Cases</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Cases</th>\n",
       "      <th>code</th>\n",
       "      <th>population</th>\n",
       "      <th>Deaths per Million</th>\n",
       "      <th>Cases per Million</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>world</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>1548</td>\n",
       "      <td>41814</td>\n",
       "      <td>AFG</td>\n",
       "      <td>38.928341</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>2020-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>world</td>\n",
       "      <td>Albania</td>\n",
       "      <td>7</td>\n",
       "      <td>421</td>\n",
       "      <td>543</td>\n",
       "      <td>22721</td>\n",
       "      <td>ALB</td>\n",
       "      <td>2.877800</td>\n",
       "      <td>189.0</td>\n",
       "      <td>7900.0</td>\n",
       "      <td>2020-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>12</td>\n",
       "      <td>642</td>\n",
       "      <td>2011</td>\n",
       "      <td>60169</td>\n",
       "      <td>DZA</td>\n",
       "      <td>43.851043</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>2020-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>world</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>5135</td>\n",
       "      <td>AND</td>\n",
       "      <td>0.077265</td>\n",
       "      <td>971.0</td>\n",
       "      <td>66460.0</td>\n",
       "      <td>2020-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world</td>\n",
       "      <td>Angola</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>299</td>\n",
       "      <td>12102</td>\n",
       "      <td>AGO</td>\n",
       "      <td>32.866268</td>\n",
       "      <td>9.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>2020-11-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group         area  Daily Deaths  Daily Cases  Deaths  Cases code  \\\n",
       "0  world  Afghanistan             4           86    1548  41814  AFG   \n",
       "1  world      Albania             7          421     543  22721  ALB   \n",
       "2  world      Algeria            12          642    2011  60169  DZA   \n",
       "3  world      Andorra             0           90      75   5135  AND   \n",
       "4  world       Angola             3          289     299  12102  AGO   \n",
       "\n",
       "   population  Deaths per Million  Cases per Million       date  \n",
       "0   38.928341                40.0             1070.0 2020-11-05  \n",
       "1    2.877800               189.0             7900.0 2020-11-05  \n",
       "2   43.851043                46.0             1370.0 2020-11-05  \n",
       "3    0.077265               971.0            66460.0 2020-11-05  \n",
       "4   32.866268                 9.0              370.0 2020-11-05  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_summary_table(df_all, last_date).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code within the modules\n",
    "\n",
    "The `CasesModel` and `DeathsModel` class are placed in the `models.py` file. The `PrepareData` class and `combine_all_data` and `create_summary_table` functions are placed in the `prepare.py` file. In the next chapter, we'll run all of our code for the entire project to prepare the data, make predictions, and save the final tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
